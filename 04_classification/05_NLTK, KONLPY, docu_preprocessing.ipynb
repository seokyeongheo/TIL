{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _NLTK 자연어 처리 패키지_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/quartz/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/quartz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/quartz/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package reuters to /Users/quartz/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quartz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package webtext to /Users/quartz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/quartz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"gutenberg\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"webtext\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "print(emma_raw[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(emma_raw[:1000])[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.', 'Sixteen', 'years', 'had', 'Miss', 'Taylor', 'been', 'in', 'Mr.', 'Woodhouse', \"'s\", 'family', ',', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', ',', 'very', 'fond', 'of', 'both', 'daughters', ',', 'but', 'particularly', 'of', 'Emma', '.', 'Between', '_them_', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.', 'Even', 'before', 'Miss', 'Taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', ',', 'the', 'mildness', 'o']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(emma_raw[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "- - - \n",
    "\n",
    "- 어간 추출(stemming)\n",
    "- 원형 복원(lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['lives', 'dies', 'flies', 'died']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words :  ['lives', 'dies', 'flies', 'died']\n",
      "PorterStemmer :  ['live', 'die', 'fli', 'die']\n",
      "LancasterStemmer :  ['liv', 'die', 'fli', 'died']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words : \", words)\n",
    "print(\"PorterStemmer : \", [ps.stem(word) for word in words])\n",
    "print(\"LancasterStemmer : \", [ls.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원형 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['live', 'die', 'fly', 'die']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lm.lemmatize(word, pos='v') for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('volumne', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('chapter', 'VBP'),\n",
       " ('1', 'CD'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('boy', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x = [\"volumne\", \"I\", \"chapter\", \"1\", \"I\", \"am\", \"a\", \"boy\", \".\"]\n",
    "tagged_list = pos_tag(x)\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volumne', 'I', 'chapter', '1', 'I', 'am', 'a', 'boy', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _KoNLPy 한국어 처리 패키지_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constitution.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolaw.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = kolaw.open('constitution.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국헌법\n",
      "\n",
      "유구한 역사와 전통에 빛나는 우리 대한국민은 3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한\n"
     ]
    }
   ],
   "source": [
    "print(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1809896.txt',\n",
       " '1809897.txt',\n",
       " '1809895.txt',\n",
       " '1809894.txt',\n",
       " '1809890.txt',\n",
       " '1809891.txt',\n",
       " '1809893.txt',\n",
       " '1809892.txt',\n",
       " '1809899.txt',\n",
       " '1809898.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobill.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행정절차법 일부개정법률안\n",
      "\n",
      "(유선호의원 대표발의 )\n",
      "\n",
      " 의 안\n",
      " 번 호\n",
      "\n",
      "9896\n",
      "\n",
      "발의연월일 : 2010.  11.  15.\n",
      "\n",
      "발  의  자 : 유선호․강기갑․김효석  \n",
      "\n",
      "최문순\n"
     ]
    }
   ],
   "source": [
    "d = kobill.open('1809896.txt').read()\n",
    "print(d[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "hannanum = Hannanum()\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 꼬꼬마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한',\n",
       " '대한민국',\n",
       " '대한민국헌법',\n",
       " '민국',\n",
       " '헌법',\n",
       " '유구',\n",
       " '역사',\n",
       " '전통',\n",
       " '우리',\n",
       " '국민',\n",
       " '3',\n",
       " '1',\n",
       " '1운동',\n",
       " '운동',\n",
       " '건립',\n",
       " '대한민국임시정부',\n",
       " '임시',\n",
       " '정부',\n",
       " '법통',\n",
       " '불의',\n",
       " '항거']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.nouns(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한민국',\n",
       " '헌법',\n",
       " '유구',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '역사',\n",
       " '와',\n",
       " '전통',\n",
       " '에',\n",
       " '빛나',\n",
       " '는',\n",
       " '우리',\n",
       " '대하',\n",
       " 'ㄴ',\n",
       " '국민',\n",
       " '은',\n",
       " '3',\n",
       " '·',\n",
       " '1',\n",
       " '운동',\n",
       " '으로',\n",
       " '건립',\n",
       " '되',\n",
       " 'ㄴ',\n",
       " '대한민국',\n",
       " '임시',\n",
       " '정부',\n",
       " '의',\n",
       " '법통',\n",
       " '과',\n",
       " '불의',\n",
       " '에',\n",
       " '항거',\n",
       " '하',\n",
       " 'ㄴ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.morphs(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국', 'NNG'),\n",
       " ('헌법', 'NNG'),\n",
       " ('유구', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('역사', 'NNG'),\n",
       " ('와', 'JC'),\n",
       " ('전통', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('빛나', 'VV'),\n",
       " ('는', 'ETD'),\n",
       " ('우리', 'NNM'),\n",
       " ('대하', 'VV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('국민', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('3', 'NR'),\n",
       " ('·', 'SP'),\n",
       " ('1', 'NR'),\n",
       " ('운동', 'NNG'),\n",
       " ('으로', 'JKM'),\n",
       " ('건립', 'NNG'),\n",
       " ('되', 'XSV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('대한민국', 'NNG'),\n",
       " ('임시', 'NNG'),\n",
       " ('정부', 'NNG'),\n",
       " ('의', 'JKG'),\n",
       " ('법통', 'NNG'),\n",
       " ('과', 'JC'),\n",
       " ('불의', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('항거', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('ㄴ', 'ETD')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(c[:66])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한민국헌법',\n",
       " '유구',\n",
       " '역사',\n",
       " '전통',\n",
       " '빛',\n",
       " '우리',\n",
       " '대한국민',\n",
       " '3·1운동',\n",
       " '건립',\n",
       " '대한민국임시정부',\n",
       " '법통',\n",
       " '불의',\n",
       " '항거한']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.nouns(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한민국헌법',\n",
       " '유구',\n",
       " '하',\n",
       " 'ㄴ',\n",
       " '역사',\n",
       " '와',\n",
       " '전통',\n",
       " '에',\n",
       " '빛',\n",
       " '나는',\n",
       " '우리',\n",
       " '대한국민',\n",
       " '은',\n",
       " '3·1운동',\n",
       " '으로',\n",
       " '건립',\n",
       " '되',\n",
       " 'ㄴ',\n",
       " '대한민국임시정부',\n",
       " '의',\n",
       " '법통',\n",
       " '과',\n",
       " '불의',\n",
       " '에',\n",
       " '항거한']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.morphs(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국헌법', 'N'),\n",
       " ('유구', 'N'),\n",
       " ('하', 'X'),\n",
       " ('ㄴ', 'E'),\n",
       " ('역사', 'N'),\n",
       " ('와', 'J'),\n",
       " ('전통', 'N'),\n",
       " ('에', 'J'),\n",
       " ('빛', 'N'),\n",
       " ('나는', 'J'),\n",
       " ('우리', 'N'),\n",
       " ('대한국민', 'N'),\n",
       " ('은', 'J'),\n",
       " ('3·1운동', 'N'),\n",
       " ('으로', 'J'),\n",
       " ('건립', 'N'),\n",
       " ('되', 'X'),\n",
       " ('ㄴ', 'E'),\n",
       " ('대한민국임시정부', 'N'),\n",
       " ('의', 'J'),\n",
       " ('법통', 'N'),\n",
       " ('과', 'J'),\n",
       " ('불의', 'N'),\n",
       " ('에', 'J'),\n",
       " ('항거한', 'N')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.pos(c[:66])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한민국',\n",
       " '헌법',\n",
       " '유구',\n",
       " '역사',\n",
       " '전통',\n",
       " '우리',\n",
       " '대한',\n",
       " '국민',\n",
       " '운동',\n",
       " '건립',\n",
       " '대한민국',\n",
       " '임시정부',\n",
       " '법',\n",
       " '통과',\n",
       " '불의',\n",
       " '항거']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.nouns(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대한민국',\n",
       " '헌법',\n",
       " '유구',\n",
       " '한',\n",
       " '역사',\n",
       " '와',\n",
       " '전통',\n",
       " '에',\n",
       " '빛나는',\n",
       " '우리',\n",
       " '대한',\n",
       " '국민',\n",
       " '은',\n",
       " '3',\n",
       " '·',\n",
       " '1',\n",
       " '운동',\n",
       " '으로',\n",
       " '건립',\n",
       " '된',\n",
       " '대한민국',\n",
       " '임시정부',\n",
       " '의',\n",
       " '법',\n",
       " '통과',\n",
       " '불의',\n",
       " '에',\n",
       " '항거',\n",
       " '한']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대한민국', 'Noun'),\n",
       " ('헌법', 'Noun'),\n",
       " ('유구', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('역사', 'Noun'),\n",
       " ('와', 'Josa'),\n",
       " ('전통', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('빛나는', 'Verb'),\n",
       " ('우리', 'Noun'),\n",
       " ('대한', 'Noun'),\n",
       " ('국민', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('3', 'Number'),\n",
       " ('·', 'Foreign'),\n",
       " ('1', 'Number'),\n",
       " ('운동', 'Noun'),\n",
       " ('으로', 'Josa'),\n",
       " ('건립', 'Noun'),\n",
       " ('된', 'Verb'),\n",
       " ('대한민국', 'Noun'),\n",
       " ('임시정부', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('법', 'Noun'),\n",
       " ('통과', 'Noun'),\n",
       " ('불의', 'Noun'),\n",
       " ('에', 'Josa'),\n",
       " ('항거', 'Noun'),\n",
       " ('한', 'Josa')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.pos(c[:66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Scikit-Learn의 문서 전처리 기능_\n",
    "- - -\n",
    "\n",
    "- DictVectorizer\n",
    "- CountVectorizer\n",
    "- TfidVectorizer\n",
    "- HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [0., 3., 2.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [{'A' : 1, 'B' : 2}, {'B' : 3, \"C\" : 2}]\n",
    "dv.fit_transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "- - -\n",
    "\n",
    "- stop_words : 문자열, 리스트, None (디폴트). 문서에서 단어장을 생성할 때 무시할 수 있는 단어. \n",
    "- analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수. \n",
    "- token_pattern : string. 토큰 정의용 정규 표현식. \n",
    "- tokenizer : 함수 또는 None (디폴트). 토큰 생성 함수.\n",
    "- ngram_range : (min_n, max_n) 튜플."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first document.',\n",
       " 'This is the second second document.',\n",
       " 'And the third one.',\n",
       " 'Is this the first document?',\n",
       " 'The last document?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [    \n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?'\n",
    "         ]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(corpus)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['This is the first document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"])\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 : analyzer, token_pattern, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer='char').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 5,\n",
       " 'the': 9,\n",
       " 'first': 4,\n",
       " 'document': 3,\n",
       " '.': 0,\n",
       " 'second': 8,\n",
       " 'and': 2,\n",
       " 'third': 10,\n",
       " 'one': 7,\n",
       " '?': 1,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram\n",
    "- - -\n",
    "\n",
    "- 단어장 생성에 사용할 크기 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 9,\n",
       " 'is': 3,\n",
       " 'the': 7,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 6,\n",
       " 'and': 0,\n",
       " 'third': 8,\n",
       " 'one': 5,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 21,\n",
       " 'is': 5,\n",
       " 'the': 14,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'this is': 22,\n",
       " 'is the': 6,\n",
       " 'the first': 15,\n",
       " 'first document': 4,\n",
       " 'second': 11,\n",
       " 'the second': 17,\n",
       " 'second second': 13,\n",
       " 'second document': 12,\n",
       " 'and': 0,\n",
       " 'third': 19,\n",
       " 'one': 10,\n",
       " 'and the': 1,\n",
       " 'the third': 18,\n",
       " 'third one': 20,\n",
       " 'is this': 7,\n",
       " 'this the': 23,\n",
       " 'last': 8,\n",
       " 'the last': 16,\n",
       " 'last document': 9}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 12,\n",
       " 'is the': 2,\n",
       " 'the first': 7,\n",
       " 'first document': 1,\n",
       " 'the second': 9,\n",
       " 'second second': 6,\n",
       " 'second document': 5,\n",
       " 'and the': 0,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'is this': 3,\n",
       " 'this the': 13,\n",
       " 'the last': 8,\n",
       " 'last document': 4}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 3, 'is': 2, 'first': 1, 'document': 0}, None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "tv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.37 s, sys: 123 ms, total: 4.5 s\n",
      "Wall time: 4.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv.fit_transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.46 s, sys: 67.4 ms, total: 3.52 s\n",
      "Wall time: 3.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1787304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hv.fit_transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
